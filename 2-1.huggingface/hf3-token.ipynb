{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9PwmaTCPpYFk7ho93mU71"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7hO-SliRplRl"},"outputs":[],"source":["!pip install transformers --upgrade\n","!pip install datasets\n"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"yAfGGK0cp2G-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# 사전 학습모델 및 토크나이저 로딩\n","model = AutoModelForCausalLM.from_pretrained('model-name')\n","tokenizer = AutoTokenizer.from_pretrained('model-name')\n","\n","# 토큰 추가\n","new_tokens = ['newword1', 'newword2']\n","tokenizer.add_tokens(new_tokens)\n","\n","# 임베딩 공간 리사이즈\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# 추가된 토큰과 함께 파인튜닝.\n","# (fine-tuning code here)"],"metadata":{"id":"BVmRMUx9p3RV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n","\n","tokenizer = Tokenizer(models.BPE()) # 토큰화 얻기\n","\n","tokenizer.pre_tokenizer = pre_tokenizers.Whitespace() # 사용자 토큰 처리 객체\n","def custom_pre_tokenizer(sequence): # 사용자 토큰 정의\n","    # Define rules to combine tokens, e.g., \"new word\" -> \"newword\"\n","    combined_sequence = sequence.replace(\"new word\", \"newword\")\n","    return combined_sequence\n","\n","# 토큰 훈련. custom pre-tokenizer 활용함.\n","trainer = trainers.BpeTrainer()\n","tokenizer.train(files=[\"path/to/training/data.txt\"], trainer=trainer, pre_tokenizer=custom_pre_tokenizer)\n","\n","# 훈련된 토큰 저장\n","tokenizer.save(\"path/to/customized_tokenizer.json\")\n"],"metadata":{"id":"XdjLbOkkqCGT"},"execution_count":null,"outputs":[]}]}
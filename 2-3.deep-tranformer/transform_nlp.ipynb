{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaTU23Ucufii",
        "outputId": "e462e41a-0036-41fd-f26c-a9469596fe46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.1.0\n",
            "  Using cached torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Using cached triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.5.82)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "INFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "  Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "  Downloading torchtext-0.17.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "  Downloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "  Downloading torchtext-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "  Downloading torchtext-0.16.1-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "  Downloading torchtext-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting torchdata==0.7.0 (from torchtext)\n",
            "  Downloading torchdata-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.0->torchtext) (2.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.16.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.0.0\n",
            "    Uninstalling triton-2.0.0:\n",
            "      Successfully uninstalled triton-2.0.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0\n",
            "    Uninstalling torch-2.0.0:\n",
            "      Successfully uninstalled torch-2.0.0\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.0\n",
            "    Uninstalling torchdata-0.6.0:\n",
            "      Successfully uninstalled torchdata-0.6.0\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.1\n",
            "    Uninstalling torchtext-0.15.1:\n",
            "      Successfully uninstalled torchtext-0.15.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 torchdata-0.7.0 torchtext-0.16.0 triton-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.1.0 transformers portalocker datasets torchtext numpy\n",
        "# !pip install torch==2.1.0 transformers==4.51.3 portalocker==3.1.1 datasets==3.5.0 torchtext==0.15.1 numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "key = userdata.get('hf-api')\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=key)"
      ],
      "metadata": {
        "id": "v-5dayQXujWD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math, matplotlib.pyplot as plt, os, numpy\n",
        "import torch, torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "from torch import Tensor\n",
        "from torch.nn import Transformer # https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/transformer.py, https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/activation.py\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from timeit import default_timer as timer"
      ],
      "metadata": {
        "id": "ZjSimtRvujfF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHjSQku2ujkY",
        "outputId": "de1dfc7a-789a-418a-816e-5a62736c7366"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "text_transform = {}\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "NUM_EPOCHS = 2 # 5 18 BEST # 50"
      ],
      "metadata": {
        "id": "u_gIldaRujmo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# current date and time\n",
        "current_date_time_string = str(datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n"
      ],
      "metadata": {
        "id": "Y1PNUQHvzJ9F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUCr6YZ60CFR",
        "outputId": "9045d9ab-2d6c-427a-dd5b-f7a66d6fcc39"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Place-holders\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "print(token_transform[SRC_LANGUAGE](\"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "print(token_transform[TGT_LANGUAGE](\"A group of people are standing in front of an igloo .\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FML6bSMpzJ_0",
        "outputId": "7e1063ae-d4c6-4231-879e-efae0d0c9739"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Eine', 'Gruppe', 'von', 'Menschen', 'steht', 'vor', 'einem', 'Iglu', '.']\n",
            "['A', 'group', 'of', 'people', 'are', 'standing', 'in', 'front', 'of', 'an', 'igloo', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "\tlanguage_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "\tfor data_sample in data_iter:\n",
        "\t\tyield token_transform[language](data_sample[language_index[language]])\n"
      ],
      "metadata": {
        "id": "fo78lprKzKJh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special symbols and indices. Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "for i, (src, tgt) in enumerate(train_iter):\n",
        "\tif i >= 5:\n",
        "\t\tbreak\n",
        "\tprint(f\"Source[{i}]: {src}\")\n",
        "\tprint(f\"Target[{i}]: {tgt}\")\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "\t# Create torchtext's Vocab object\n",
        "\tvocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tmin_freq=1,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tspecials=special_symbols,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tspecial_first=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qeEQn6FzKCC",
        "outputId": "254bb9b2-6659-4e71-8874-680e3e19dcbe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source[0]: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
            "Target[0]: Two young, White males are outside near many bushes.\n",
            "Source[1]: Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
            "Target[1]: Several men in hard hats are operating a giant pulley system.\n",
            "Source[2]: Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
            "Target[2]: A little girl climbing into a wooden playhouse.\n",
            "Source[3]: Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
            "Target[3]: A man in a blue shirt is standing on a ladder cleaning a window.\n",
            "Source[4]: Zwei Männer stehen am Herd und bereiten Essen zu.\n",
            "Target[4]: Two men are at the stove preparing food.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "\tvocab_transform[ln].set_default_index(UNK_IDX)\n",
        "torch.manual_seed(0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LSbjuQYzKEg",
        "outputId": "dab7cb69-1cf6-42a6-c281-0eaec9280961"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7b2d1851a490>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "\tglobal text_transform\n",
        "\n",
        "\tsrc_batch, tgt_batch = [], []\n",
        "\tfor src_sample, tgt_sample in batch:\n",
        "\t\tsrc_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "\t\ttgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "\tsrc_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "\ttgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "\treturn src_batch, tgt_batch\n"
      ],
      "metadata": {
        "id": "hvtR0zc5zhSY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "\tdef func(txt_input):\n",
        "\t\tfor transform in transforms:\n",
        "\t\t\ttxt_input = transform(txt_input)\n",
        "\t\treturn txt_input\n",
        "\treturn func\n"
      ],
      "metadata": {
        "id": "9U2sV1zJzhNY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "\treturn torch.cat((torch.tensor([BOS_IDX]),\n",
        "\t\t\t\t\ttorch.tensor(token_ids),\n",
        "\t\t\t\t\ttorch.tensor([EOS_IDX])))\n"
      ],
      "metadata": {
        "id": "3rIV1DBKzhP0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "\ttext_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "\t\t\t\t\t\t\t\t\t\t\tvocab_transform[ln], #Numericalization\n",
        "\t\t\t\t\t\t\t\t\t\t\ttensor_transform) # Add BOS/EOS and create tensor\n"
      ],
      "metadata": {
        "id": "kHiFOlXPzKHC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "\tdef __init__(self,\n",
        "\t\t\t\temb_size: int,\n",
        "\t\t\t\tdropout: float,\n",
        "\t\t\t\tmaxlen: int = 5000):\n",
        "\t\tsuper(PositionalEncoding, self).__init__()\n",
        "\t\tden = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "\t\tpos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "\t\tpos_embedding = torch.zeros((maxlen, emb_size))\n",
        "\t\tpos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "\t\tpos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "\t\tpos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "\t\tself.dropout = nn.Dropout(dropout)\n",
        "\t\tself.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "\tdef forward(self, token_embedding: Tensor):\n",
        "\t\treturn self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n"
      ],
      "metadata": {
        "id": "bOmFMX4vzKLx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "\tdef __init__(self, vocab_size: int, emb_size):\n",
        "\t\tsuper(TokenEmbedding, self).__init__()\n",
        "\t\tself.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "\t\tself.emb_size = emb_size\n",
        "\n",
        "\tdef forward(self, tokens: Tensor):\n",
        "\t\treturn self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n"
      ],
      "metadata": {
        "id": "6p00MT_ezYu8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "\tdef __init__(self,\n",
        "\t\t\t\tnum_encoder_layers: int,\n",
        "\t\t\t\tnum_decoder_layers: int,\n",
        "\t\t\t\temb_size: int,\n",
        "\t\t\t\tnhead: int,\n",
        "\t\t\t\tsrc_vocab_size: int,\n",
        "\t\t\t\ttgt_vocab_size: int,\n",
        "\t\t\t\tdim_feedforward: int = 512,\n",
        "\t\t\t\tdropout: float = 0.1):\n",
        "\t\tsuper(Seq2SeqTransformer, self).__init__()\n",
        "\t\tself.transformer = Transformer(d_model=emb_size,\n",
        "\t\t\t\t\t\t\t\t\tnhead=nhead,\n",
        "\t\t\t\t\t\t\t\t\tnum_encoder_layers=num_encoder_layers,\n",
        "\t\t\t\t\t\t\t\t\tnum_decoder_layers=num_decoder_layers,\n",
        "\t\t\t\t\t\t\t\t\tdim_feedforward=dim_feedforward,\n",
        "\t\t\t\t\t\t\t\t\tdropout=dropout)\n",
        "\t\tself.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\t\tself.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "\t\tself.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "\t\tself.positional_encoding = PositionalEncoding(\n",
        "\t\t\temb_size, dropout=dropout)\n",
        "\n",
        "\tdef forward(self,\n",
        "\t\t\t\tsrc: Tensor,\n",
        "\t\t\t\ttrg: Tensor,\n",
        "\t\t\t\tsrc_mask: Tensor,\n",
        "\t\t\t\ttgt_mask: Tensor,\n",
        "\t\t\t\tsrc_padding_mask: Tensor,\n",
        "\t\t\t\ttgt_padding_mask: Tensor,\n",
        "\t\t\t\tmemory_key_padding_mask: Tensor):\n",
        "\t\tsrc_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "\t\ttgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "\t\touts = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "\t\t\t\t\t\t\t\tsrc_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "\t\treturn self.generator(outs)\n",
        "\n",
        "\tdef encode(self, src: Tensor, src_mask: Tensor):\n",
        "\t\treturn self.transformer.encoder(self.positional_encoding(\n",
        "\t\t\t\t\t\t\tself.src_tok_emb(src)), src_mask)\n",
        "\n",
        "\tdef decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "\t\treturn self.transformer.decoder(self.positional_encoding(\n",
        "\t\t\t\t\t\tself.tgt_tok_emb(tgt)), memory,\n",
        "\t\t\t\t\t\ttgt_mask)\n"
      ],
      "metadata": {
        "id": "wJeRMIC-zYxU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz): # create diagonal matrix\n",
        "\tmask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "\tmask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "\treturn mask\n",
        "\n"
      ],
      "metadata": {
        "id": "qfzsTzFCzYzw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(src, tgt):\n",
        "\tsrc_seq_len = src.shape[0]\n",
        "\ttgt_seq_len = tgt.shape[0]\n",
        "\n",
        "\ttgt_mask = generate_square_subsequent_mask(tgt_seq_len).to(DEVICE)\n",
        "\tsrc_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool) # don't train future token\n",
        "\n",
        "\tsrc_padding_mask = (src == PAD_IDX).transpose(0, 1) # padding mask\n",
        "\ttgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1) # padding mask\n",
        "\treturn src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
      ],
      "metadata": {
        "id": "gVR4cv4uzKis"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(val_dataloader, model, loss_fn):\n",
        "\tmodel.eval()\n",
        "\tlosses = 0\n",
        "\n",
        "\tfor src, tgt in tqdm(val_dataloader):\n",
        "\t\tsrc = src.to(DEVICE)\n",
        "\t\ttgt = tgt.to(DEVICE)\n",
        "\n",
        "\t\ttgt_input = tgt[:-1, :]\n",
        "\t\tsrc_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "\t\tlogits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "\t\ttgt_out = tgt[1:, :]\n",
        "\t\tloss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "\t\tlosses += loss.item()\n",
        "\n",
        "\treturn losses / len(list(val_dataloader))\n"
      ],
      "metadata": {
        "id": "7BcNKhD-zhUo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "\tsrc = src.to(DEVICE)\n",
        "\tsrc_mask = src_mask.to(DEVICE)\n",
        "\n",
        "\tmemory = model.encode(src, src_mask)\n",
        "\tys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "\tfor i in range(max_len-1):\n",
        "\t\tmemory = memory.to(DEVICE)\n",
        "\t\ttgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "\t\t\t\t\t.type(torch.bool)).to(DEVICE)\n",
        "\t\tout = model.decode(ys, memory, tgt_mask)\n",
        "\t\tout = out.transpose(0, 1)\n",
        "\t\tprob = model.generator(out[:, -1])\n",
        "\t\t_, next_word = torch.max(prob, dim=1)\n",
        "\t\tnext_word = next_word.item()\n",
        "\n",
        "\t\tys = torch.cat([ys,\n",
        "\t\t\t\t\t\ttorch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "\t\tif next_word == EOS_IDX:\n",
        "\t\t\tbreak\n",
        "\treturn ys\n"
      ],
      "metadata": {
        "id": "QXrwZoPdzmcg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "\tglobal text_transform\n",
        "\n",
        "\tmodel.eval()\n",
        "\tsrc = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1).to(DEVICE)\n",
        "\tnum_tokens = src.shape[0]\n",
        "\tsrc_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE)\n",
        "\ttgt_tokens = greedy_decode(\n",
        "\t\tmodel,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "\treturn \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n"
      ],
      "metadata": {
        "id": "tou1B2uYzmgU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_dataloader, BATCH_SIZE, model, optimizer, loss_fn):\n",
        "\tmodel.train()\n",
        "\tlosses = 0\n",
        "\n",
        "\tfor index, (src, tgt) in tqdm(enumerate(train_dataloader)):\n",
        "\t\tsrc = src.to(DEVICE)  # (29,128)\n",
        "\t\ttgt = tgt.to(DEVICE)  # (36,128)\n",
        "\t\ttgt_input = tgt[:-1, :]\n",
        "\t\tsrc_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "\t\tlogits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\ttgt_out = tgt[1:, :] # tgt(sequence, batch)=(36,128). shift sequence left side=(35,128)\n",
        "\t\tloss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)) # (35*128,voca token prob)=(4480, 10837), tgt_out(4480). loss = max_index of logits's each row is predicted token - tgt_out's each element\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\toptimizer.step()\n",
        "\t\tlosses += loss.item()\n",
        "\n",
        "\treturn losses / len(list(train_dataloader))\n"
      ],
      "metadata": {
        "id": "UewmpzkEzmjF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "\tglobal token_tranform, vocab_transform\n",
        "\tglobal PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX\n",
        "\tglobal SRC_VOCAB_SIZE, TGT_VOCAB_SIZE\n",
        "\tglobal NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, FFN_HID_DIM\n",
        "\n",
        "\ttransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "\t\t\t\t\t\t\t\t\tNHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "\tfor p in transformer.parameters():\n",
        "\t\tif p.dim() > 1:\n",
        "\t\t\tnn.init.xavier_uniform_(p)\n",
        "\n",
        "\ttransformer = transformer.to(DEVICE)\n",
        "\tloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\toptimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "\t# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "\tglobal text_transform\n",
        "\n",
        "\t# current path of this module\n",
        "\tmodule_path = \".\" # os.path.dirname(os.path.abspath(__file__))\n",
        "\twriter = SummaryWriter(log_dir=os.path.join(module_path, \"logs\"))\n",
        "\ttrain_losses = []\n",
        "\tval_losses = []\n",
        "\tfor epoch in range(1, NUM_EPOCHS+1):\n",
        "\t\tstart_time = timer()\n",
        "\t\ttrain_loss = train_epoch(train_dataloader, BATCH_SIZE, transformer, optimizer, loss_fn)\n",
        "\t\ttrain_losses.append(train_loss)\n",
        "\t\tend_time = timer()\n",
        "\t\tval_loss = evaluate(val_dataloader, transformer, loss_fn)\n",
        "\t\tval_losses.append(val_loss)\n",
        "\t\twriter.add_scalar(\"Train Loss\", train_loss, epoch)\n",
        "\t\twriter.add_scalar(\"Validation Loss\", val_loss, epoch)\n",
        "\t\tprint((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "\twriter.close()\n",
        "\n",
        "\tmodel_file = module_path + '/' + current_date_time_string + '_transformer_model.pth'\n",
        "\ttorch.save(transformer.state_dict(), model_file)\n",
        "\tprint(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "\n",
        "\t# plot\n",
        "\tplt.xlabel(\"Epoch\")\n",
        "\tplt.ylabel(\"Loss\")\n",
        "\tplt.legend()\n",
        "\tplt.plot(range(1, NUM_EPOCHS+1), train_losses, label=\"Train Loss\")\n",
        "\tplt.show()\n",
        "\n",
        "\tplt.legend()\n",
        "\tplt.plot(range(1, NUM_EPOCHS+1), val_losses, label=\"Validation Loss\")\n",
        "\tplt.show()\n",
        "\n",
        "\treturn model_file\n"
      ],
      "metadata": {
        "id": "Sz4hSk8UzhXA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model_file):\n",
        "\tglobal token_tranform, vocab_transform, text_transform\n",
        "\tglobal PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX\n",
        "\tglobal SRC_VOCAB_SIZE, TGT_VOCAB_SIZE\n",
        "\tglobal NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, FFN_HID_DIM\n",
        "\n",
        "\ttransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "\t\t\t\t\t\t\t\t\tNHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "\t# load transformer model\n",
        "\tpth = torch.load(model_file)\n",
        "\ttransformer.load_state_dict(pth)\n",
        "\ttransformer.eval()\n",
        "\ttransformer = transformer.to(DEVICE)\n",
        "\n",
        "\tprint(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "\n",
        "\tval_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\tfor i, (src, tgt) in enumerate(val_iter):\n",
        "\t\tif i >= 5:\n",
        "\t\t\tbreak\n",
        "\t\tprint(f\"Source[{i}]: {src}\")\n",
        "\t\tprint(f\"Target[{i}]: {tgt}\")\n",
        "\n",
        "\t\toutput = translate(transformer, src)\n",
        "\t\tprint(f\"Predicted[{i}]: {output}\")\n",
        "\n",
        "\tinput()\n"
      ],
      "metadata": {
        "id": "kczpPaR_zhZ2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = train_model()\n",
        "test_model(fname)\n",
        "# test_model('./20250420_0945_transformer_model.pth')\n",
        "# test_model('./20250419_2309_transformer_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_RT6AckztyV",
        "outputId": "139a1c78-c3e8-425c-8747-2d9884c3a142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "227it [00:40,  5.55it/s]\n",
            "8it [00:00, 11.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.344, Val loss: 4.106, Epoch time = 47.343s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "81it [00:13,  6.00it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V8s_xqsqzt3M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}